import cv2
import os
import torch
import numpy as np
import subprocess
from pathlib import Path
import sys
import time
import socket
import json
from torchvision.ops import nms
import threading
from collections import deque
import queue

MODEL_PATH = Path(r"C:\Users\swx20\Desktop\yolov11\bestbin.onnx")
if not os.path.exists(MODEL_PATH):
    print(f"模型文件不存在: {MODEL_PATH}")
    sys.exit(1)
else:
    print(f"模型文件存在: {MODEL_PATH}")

RDK_X5_IP = "192.168.244.86"
RTSP_PORT = 8554
RTMP_PORT = 1935
CONF_THRESHOLD = 0.5
IOU_THRESHOLD = 0.5
SQUARE_SIZE = 640
FRAME_SIZE = (SQUARE_SIZE, SQUARE_SIZE)
MODEL_INPUT_WIDTH = SQUARE_SIZE
MODEL_INPUT_HEIGHT = SQUARE_SIZE
RTMP_URL = f"rtmp://{RDK_X5_IP}:{RTMP_PORT}/live/mystream"
device = torch.device('cpu')

HOST = "192.168.244.141"
PORT = 8080

ENABLE_PERFORMANCE_MONITOR = True
PERFORMANCE_MONITOR_INTERVAL = 5

FRAME_BUFFER_SIZE = 10
PUSH_BUFFER_SIZE = 5

MIN_BOX_AREA = 500
FILL_AREA_SCALE = 0.7
MAX_FILL_RATE = 100

def scale_coords(img1_shape, coords, img0_shape):
    gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])
    pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2

    coords[:, [0, 2]] -= pad[0]
    coords[:, [1, 3]] -= pad[1]
    coords[:, :4] /= gain

    coords[:, [0, 2]] = coords[:, [0, 2]].clip(0, img0_shape[1])
    coords[:, [1, 3]] = coords[:, [1, 3]].clip(0, img0_shape[0])
    return coords

class RDKCamera:
    def __init__(self):
        self.retry_count = 5
        self.rtsp_url = f"rtsp://{RDK_X5_IP}:{RTSP_PORT}/mystream"
        self.cap = None
        self.buffer = deque(maxlen=FRAME_BUFFER_SIZE)
        self.running = False
        self.thread = None
        self.last_frame_time = 0
        self.frame_counter = 0
        self.fps = 0
        
        self.start()

    def _connect(self):
        if self.cap is None or not self.cap.isOpened():
            print(f"尝试连接摄像头: {self.rtsp_url}")
            self.cap = cv2.VideoCapture(self.rtsp_url, cv2.CAP_FFMPEG)
            if not self.cap.isOpened():
                raise RuntimeError(f"无法连接摄像头: {self.rtsp_url}")
            
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_SIZE[0])
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_SIZE[1])
            self.cap.set(cv2.CAP_PROP_FPS, 15)
            print("摄像头连接成功，已设置为正方形输出")

    def _read_frames(self):
        while self.running:
            try:
                self._connect()
                
                ret, frame = self.cap.read()
                if not ret:
                    print("读取帧失败，尝试重新连接")
                    self.cap.release()
                    self.cap = None
                    time.sleep(1)
                    continue
                
                current_time = time.time()
                if self.last_frame_time > 0:
                    frame_time = current_time - self.last_frame_time
                    self.fps = 1.0 / frame_time if frame_time > 0 else 0
                self.last_frame_time = current_time
                self.frame_counter += 1
                
                self.buffer.append(frame)
                
                time.sleep(0.01)
            except Exception as e:
                print(f"读取帧时发生错误: {e}")
                time.sleep(1)

    def start(self):
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._read_frames)
            self.thread.daemon = True
            self.thread.start()
            print("摄像头读取线程已启动")

    def get_frame(self):
        if len(self.buffer) > 0:
            return self.buffer[-1]
        return None

    def release(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.cap and self.cap.isOpened():
            self.cap.release()
        print("摄像头资源已释放")

    def get_fps(self):
        return self.fps

class PlantDetector:
    def __init__(self, model_path, device='cpu'):
        model_path = Path(model_path)
        if not model_path.exists():
            raise FileNotFoundError(f"模型文件不存在: {model_path}")

        import onnxruntime as ort
        print("正在初始化ONNX模型...")
        session_options = ort.SessionOptions()
        session_options.intra_op_num_threads = 4
        self.session = ort.InferenceSession(str(model_path), session_options)
        
        print("\n===== 模型输入信息 =====")
        for i, input_info in enumerate(self.session.get_inputs()):
            print(f"  输入 {i}: {input_info.name}, 形状: {input_info.shape}")
        
        print("\n===== 模型输出信息 =====")
        for i, output_info in enumerate(self.session.get_outputs()):
            print(f"  输出 {i}: {output_info.name}, 形状: {output_info.shape}")
        
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [output.name for output in self.session.get_outputs()]
        
        self.names = ['garbage', 'garbage_bin', 'overflow']
        self.colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]
        
        self.inference_time = 0
        self.detection_count = 0

    def detect(self, frame):
        if frame is None:
            return []
            
        start_time = time.time()
        
        if frame.shape[0] != MODEL_INPUT_HEIGHT or frame.shape[1] != MODEL_INPUT_WIDTH:
            frame = cv2.resize(frame, (MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT), interpolation=cv2.INTER_AREA)
        
        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = img.transpose(2, 0, 1)
        img = np.expand_dims(img, 0).astype(np.float32) / 255.0

        outputs = self.session.run(self.output_names, {self.input_name: img})
        
        self.inference_time = time.time() - start_time
        self.detection_count += 1
        
        return outputs

    def get_avg_inference_time(self):
        return self.inference_time * 1000 if self.detection_count > 0 else 0

def process_detections(outputs, frame_shape):
    detections = []
    
    for output in outputs:
        if len(output.shape) == 3:
            output = np.squeeze(output, 0)
            
            boxes = output[:, :4]
            obj_conf = output[:, 4]
            cls_probs = output[:, 5:]
            
            max_cls_scores = np.max(cls_probs, axis=1)
            cls_ids = np.argmax(cls_probs, axis=1)
            
            scores = obj_conf * max_cls_scores
            
            mask = scores > CONF_THRESHOLD
            boxes = boxes[mask]
            scores = scores[mask]
            cls_ids = cls_ids[mask]
            
            if len(boxes) == 0:
                continue
                
            xyxy_boxes = np.zeros_like(boxes)
            xyxy_boxes[:, 0] = boxes[:, 0] - boxes[:, 2] / 2
            xyxy_boxes[:, 1] = boxes[:, 1] - boxes[:, 3] / 2
            xyxy_boxes[:, 2] = boxes[:, 0] + boxes[:, 2] / 2
            xyxy_boxes[:, 3] = boxes[:, 1] + boxes[:, 3] / 2
            
            detections.append(np.column_stack([xyxy_boxes, scores, cls_ids]))
    
    if len(detections) == 0:
        return np.zeros((0, 6))
    
    detections = np.vstack(detections)
    
    if len(detections) > 0:
        boxes = torch.from_numpy(detections[:, :4])
        scores = torch.from_numpy(detections[:, 4])
        keep = nms(boxes, scores, IOU_THRESHOLD).numpy()
        detections = detections[keep]
    
    if len(detections) > 0:
        detections[:, :4] = scale_coords(
            (MODEL_INPUT_HEIGHT, MODEL_INPUT_WIDTH),
            detections[:, :4],
            frame_shape
        )
    
    return detections

def calculate_fill_rate(frame, x1, y1, x2, y2, class_id):
    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
    
    box_area = (x2 - x1) * (y2 - y1)
    
    if box_area < MIN_BOX_AREA:
        return 0, None
    
    roi = frame[y1:y2, x1:x2]
    if roi.size == 0:
        return 0, None
    
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    
    if class_id == 1:
        thresh = cv2.adaptiveThreshold(blur, 255, 
                                      cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                      cv2.THRESH_BINARY_INV, 11, 2)
    else:
        _, thresh = cv2.threshold(blur, 127, 255, cv2.THRESH_BINARY_INV)
    
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)
    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel, iterations=2)
    
    fill_area = cv2.countNonZero(closing)
    
    fill_rate = (fill_area / box_area) * 100 * FILL_AREA_SCALE
    
    fill_rate = min(fill_rate, MAX_FILL_RATE)
    
    return fill_rate, closing

class FFmpegPusher:
    def __init__(self, output_url, frame_size=FRAME_SIZE, fps=15):
        self.output_url = output_url
        self.frame_size = frame_size
        self.fps = fps
        self.process = None
        self.running = False
        self.thread = None
        self.buffer = queue.Queue(maxsize=PUSH_BUFFER_SIZE)
        self.frame_counter = 0
        self.push_errors = 0
        self.start_time = 0
        self.last_frame_time = 0
        
        self.start()

    def _init_process(self):
        ffmpeg_path = r"D:/ffmpeg/ffmpeg-2025-05-01-git-707c04fe06-full_build/bin/ffmpeg.exe"
        command = [
            ffmpeg_path,
            '-y',
            '-f', 'rawvideo',
            '-vcodec', 'rawvideo',
            '-pix_fmt', 'bgr24',
            '-s', f'{self.frame_size[0]}x{self.frame_size[1]}',
            '-r', str(self.fps),
            '-i', '-',
            '-c:v', 'libx264',
            '-preset', 'ultrafast',
            '-tune', 'zerolatency',
            '-b:v', '1000k',
            '-maxrate', '1500k',
            '-bufsize', '2000k',
            '-pix_fmt', 'yuv420p',
            '-f', 'flv',
            self.output_url
        ]
        
        try:
            print(f"启动FFmpeg推流进程: {self.output_url}")
            self.process = subprocess.Popen(command, stdin=subprocess.PIPE, 
                                           stdout=subprocess.DEVNULL, 
                                           stderr=subprocess.DEVNULL)
            self.start_time = time.time()
            self.last_frame_time = self.start_time
            return True
        except Exception as e:
            print(f"FFmpeg进程启动失败: {e}")
            self.process = None
            return False

    def _push_frames(self):
        while self.running:
            try:
                if self.process is None or self.process.poll() is not None:
                    if not self._init_process():
                        time.sleep(1)
                        continue
                
                try:
                    frame = self.buffer.get(timeout=1.0)
                except queue.Empty:
                    current_time = time.time()
                    if current_time - self.last_frame_time > 5.0:
                        frame = np.zeros((self.frame_size[1], self.frame_size[0], 3), dtype=np.uint8)
                    else:
                        continue
                
                if frame.shape[0] != self.frame_size[1] or frame.shape[1] != self.frame_size[0]:
                    frame = cv2.resize(frame, self.frame_size, interpolation=cv2.INTER_AREA)
                
                try:
                    self.process.stdin.write(frame.tobytes())
                    self.frame_counter += 1
                    self.last_frame_time = time.time()
                except Exception as e:
                    print(f"推流失败: {e}")
                    self.push_errors += 1
                    self.process.stdin.close()
                    self.process.wait()
                    self.process = None
                    time.sleep(0.5)
            except Exception as e:
                print(f"推流线程发生错误: {e}")
                time.sleep(1)

    def push_frame(self, frame):
        if not self.running:
            return False
            
        if self.buffer.full():
            try:
                self.buffer.get_nowait()
            except queue.Empty:
                pass
                
        try:
            self.buffer.put_nowait(frame)
            return True
        except queue.Full:
            print("推流缓冲区已满，丢弃帧")
            return False

    def start(self):
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._push_frames)
            self.thread.daemon = True
            self.thread.start()
            print("FFmpeg推流线程已启动")

    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.process:
            self.process.stdin.close()
            self.process.terminate()
            self.process.wait()
        print("FFmpeg推流已停止")

    def get_stats(self):
        if not self.start_time:
            return {"fps": 0, "frame_count": 0, "errors": self.push_errors}
            
        elapsed = time.time() - self.start_time
        fps = self.frame_counter / elapsed if elapsed > 0 else 0
        return {"fps": fps, "frame_count": self.frame_counter, "errors": self.push_errors}

class PerformanceMonitor:
    def __init__(self, camera, detector, pusher):
        self.camera = camera
        self.detector = detector
        self.pusher = pusher
        self.running = False
        self.thread = None
        
    def _monitor(self):
        while self.running:
            try:
                camera_stats = {"fps": self.camera.get_fps()}
                detector_stats = {"avg_inference_time": self.detector.get_avg_inference_time()}
                pusher_stats = self.pusher.get_stats()
                
                print("\n===== 性能监控 =====")
                print(f"摄像头: FPS={camera_stats['fps']:.1f}")
                print(f"检测器: 平均推理时间={detector_stats['avg_inference_time']:.1f}ms")
                print(f"推流器: FPS={pusher_stats['fps']:.1f}, 总帧数={pusher_stats['frame_count']}, 错误数={pusher_stats['errors']}")
                print("===================\n")
                
                time.sleep(PERFORMANCE_MONITOR_INTERVAL)
            except Exception as e:
                print(f"性能监控线程发生错误: {e}")
                time.sleep(PERFORMANCE_MONITOR_INTERVAL)
    
    def start(self):
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._monitor)
            self.thread.daemon = True
            self.thread.start()
            print("性能监控线程已启动")
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=1.0)

def main():
    print("===== 程序初始化开始 =====")

    print("初始化摄像头...")
    camera = RDKCamera()
    
    print("初始化目标检测器...")
    detector = PlantDetector(MODEL_PATH, device)
    
    print("初始化FFmpeg推流器...")
    pusher = FFmpegPusher(RTMP_URL, FRAME_SIZE)
    
    monitor = None
    if ENABLE_PERFORMANCE_MONITOR:
        print("初始化性能监控...")
        monitor = PerformanceMonitor(camera, detector, pusher)
        monitor.start()
    
    print("初始化Socket连接...")
    sock = None
    max_retries = 3
    for i in range(max_retries):
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(5)
            sock.connect((HOST, PORT))
            print("Socket连接成功")
            break
        except Exception as e:
            if i < max_retries - 1:
                print(f"Socket连接失败，重试 {i + 1}/{max_retries}: {e}")
                time.sleep(1)
            else:
                print(f"Socket连接失败，放弃重试: {e}")
                sock = None
    
    print("===== 程序初始化完成 =====")
    
    try:
        while True:
            frame = camera.get_frame()
            if frame is None:
                print("获取帧失败，等待中...")
                time.sleep(0.1)
                continue
                
            outputs = detector.detect(frame)
            
            detections = process_detections(outputs, frame.shape[:2])
            print(f"检测到 {len(detections)} 个目标")

            trash_data = []  
            
            for det in detections:
                x1, y1, x2, y2, conf, cls_id = det
                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
                cls_id = int(cls_id)
                
                fill_rate = 0
                fill_mask = None
                if detector.names[cls_id] in ['garbage_bin', 'overflow']:
                    fill_rate, fill_mask = calculate_fill_rate(frame, x1, y1, x2, y2, cls_id)
                
                label = f"{detector.names[cls_id]} {conf:.2f}"
                color = detector.colors[cls_id]
                
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                
                (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                cv2.rectangle(frame, (x1, y1 - label_height - 10), (x1 + label_width, y1), color, -1)
                
                cv2.putText(frame, label, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                if detector.names[cls_id] in ['garbage_bin', 'overflow']:
                    fill_text = f"填充率: {fill_rate:.1f}%"
                    (fill_width, fill_height), _ = cv2.getTextSize(fill_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                    cv2.rectangle(frame, (x1, y2 + 10), (x1 + fill_width, y2 + fill_height + 10), color, -1)
                    cv2.putText(frame, fill_text, (x1, y2 + fill_height + 5),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                trash_data.append({
                    "x": (x1 + x2) / 2,
                    "y": (y1 + y2) / 2,
                    "class": detector.names[cls_id],
                    "fill_rate": fill_rate  
                })

            fps_text = f"检测FPS: {1/detector.inference_time:.1f}" if detector.inference_time > 0 else "检测FPS: N/A"
            cv2.putText(frame, fps_text, (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            pusher.push_frame(frame)

            cv2.imshow('Detection (Square Frame)', frame)

            if sock:
                try:
                    data = json.dumps({"trash": trash_data}).encode()
                    sock.sendall(data)
                except Exception as e:
                    print(f"Socket发送失败: {e}")
                    sock.close()
                    sock = None

            if cv2.waitKey(1) == ord('q'):
                print("\n用户请求退出...")
                break

    finally:
        print("\n===== 清理资源 =====")
        if monitor:
            monitor.stop()
        camera.release()
        pusher.stop()
        cv2.destroyAllWindows()
        if sock:
            sock.close()
        print("程序退出完成")

if __name__ == "__main__":
    main()